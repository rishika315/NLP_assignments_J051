{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94veDnR-Ec2A"
   },
   "source": [
    "## **IMDB Sentiment Classification: Model Benchmarking & Fine-Tuning**\n",
    "\n",
    "We experimented with the **IMDB 50K movie reviews dataset** to perform binary sentiment classification (positive vs. negative). To identify the best lightweight model for CPU execution, we:\n",
    "\n",
    "1. **Prepared the dataset** from CSV, mapped sentiments to binary labels, and created a subset for fast experimentation.\n",
    "2. **Fine-tuned 5 transformer models** (DistilBERT, TinyBERT, SmallBERT, ALBERT, MobileBERT) on the subset using a **custom F1 score** for evaluation.\n",
    "3. **Selected the best-performing model** based on F1 score and retrained it on the **entire IMDB dataset**.\n",
    "4. **Evaluated performance** on a held-out test set and reported precision, recall, and F1.\n",
    "5. **Ran inference** on 10 randomly sampled reviews from the test set to demonstrate predictions.\n",
    "\n",
    "This pipeline balances **speed and accuracy on CPU**, ensuring quick benchmarking while still scaling to the full dataset with the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# -----------------------\n",
    "# User-tunable / speed knobs\n",
    "# -----------------------\n",
    "NUM_EXAMPLES_SUBSET = 5000   # subset to quickly compare models (train+val)\n",
    "NUM_VAL_SUBSET = 1000\n",
    "NUM_EPOCHS_SUBSET = 2       # small epochs for fast runs\n",
    "NUM_EPOCHS_FULL = 2         # when training best model on full dataset\n",
    "BATCH_SIZE = 16             # CPU-friendly (decrease if OOM)s\n",
    "SEED = 42\n",
    "MAX_LENGTH = 256            # max tokens (shorter = faster)\n",
    "NUM_THREADS = 4             # torch threads\n",
    "OUTPUT_DIR = \"imdb_finetune_output\"\n",
    "\n",
    "# 5 CPU-friendly models to compare\n",
    "MODEL_NAMES = [\n",
    "    \"distilbert-base-uncased\",        # balanced speed & quality\n",
    "    \"prajjwal1/bert-tiny\",            # tiny BERT (very fast)\n",
    "    \"prajjwal1/bert-small\",           # small BERT\n",
    "    \"albert-base-v2\",                 # parameter-efficient\n",
    "    \"google/mobilebert-uncased\"       # mobileBERT (optimized)\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Environment setup\n",
    "# -----------------------\n",
    "torch.set_num_threads(NUM_THREADS)\n",
    "set_seed(SEED)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_imdb_csv(csv_path: str = \"IMDB_Dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Expects CSV with columns 'review' and 'sentiment' (positive/negative).\n",
    "    This Kaggle dataset uses these names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"{csv_path} not found. Download the Kaggle dataset and place IMDB_Dataset.csv here.\"\n",
    "        )\n",
    "    ds = load_dataset(\"csv\", data_files=csv_path)[\"train\"]\n",
    "    # rename to standard names if necessary\n",
    "    if \"review\" not in ds.column_names or \"sentiment\" not in ds.column_names:\n",
    "        raise ValueError(\"CSV must contain 'review' and 'sentiment' columns.\")\n",
    "    # map label string to int\n",
    "    def map_label(example):\n",
    "        example[\"label\"] = 1 if example[\"sentiment\"].lower().startswith(\"pos\") else 0\n",
    "        return example\n",
    "    ds = ds.map(map_label)\n",
    "    ds = ds.remove_columns([c for c in ds.column_names if c not in (\"review\", \"label\")])\n",
    "    return ds\n",
    "\n",
    "def prepare_datasets(full_ds, subset_size=NUM_EXAMPLES_SUBSET, val_size=NUM_VAL_SUBSET):\n",
    "    \"\"\"\n",
    "    Create a quick subset for model selection:\n",
    "      sample subset_size + val_size rows, then split into train/val/test.\n",
    "    Also returns a held-out test set (20% of original).\n",
    "    \"\"\"\n",
    "    # shuffle full dataset deterministically\n",
    "    full_shuffled = full_ds.shuffle(seed=SEED)\n",
    "    # held-out test set: 10k or 20% whichever smaller\n",
    "    test_count = min(int(len(full_shuffled) * 0.2), 10000)\n",
    "    test_ds = full_shuffled.select(range(test_count))\n",
    "    # For subset training, sample from remainder\n",
    "    remaining = full_shuffled.select(range(test_count, len(full_shuffled)))\n",
    "    subset_total = subset_size + val_size\n",
    "    subset_total = min(subset_total, len(remaining))\n",
    "    subset = remaining.select(range(subset_total))\n",
    "    # split\n",
    "    train_subset = subset.select(range(subset_size))\n",
    "    val_subset = subset.select(range(subset_size, subset_total))\n",
    "    return DatasetDict({\"train\": train_subset, \"validation\": val_subset, \"test\": test_ds})\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"review\"], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "def compute_metrics_custom(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"binary\")\n",
    "    prec = precision_score(labels, preds, zero_division=0)\n",
    "    rec = recall_score(labels, preds, zero_division=0)\n",
    "    return {\"f1\": f1, \"precision\": prec, \"recall\": rec}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_model(model_name: str, datasets: DatasetDict, tokenizer=None, epochs=NUM_EPOCHS_SUBSET, run_name=None):\n",
    "    print(f\"\\n--- Training {model_name} (epochs={epochs}) ---\")\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # tokenize\n",
    "    tokenized_train = datasets[\"train\"].map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "    tokenized_val = datasets[\"validation\"].map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=datasets[\"validation\"].column_names)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # load model (num_labels=2)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUTPUT_DIR, f\"{run_name or model_name.replace('/', '_')}\"),\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",         # do not save checkpoints for subset runs (speed)\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        seed=SEED,\n",
    "        disable_tqdm=False,\n",
    "        dataloader_drop_last=False,\n",
    "        fp16=False  # CPU -> no mixed precision\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_custom\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    # clean up to free CPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return metrics[\"eval_f1\"], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_on_full(model_name: str, full_ds, tokenizer=None, epochs=NUM_EPOCHS_FULL):\n",
    "    print(f\"\\n=== Final training on full dataset with {model_name} ===\")\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # Split full dataset into train/validation/test (80/10/10)\n",
    "    ds_shuf = full_ds.shuffle(seed=SEED)\n",
    "    n = len(ds_shuf)\n",
    "    n_train = int(n * 0.8)\n",
    "    n_val = int(n * 0.1)\n",
    "    train_full = ds_shuf.select(range(0, n_train))\n",
    "    val_full = ds_shuf.select(range(n_train, n_train + n_val))\n",
    "    test_full = ds_shuf.select(range(n_train + n_val, n))\n",
    "    tokenized_train = train_full.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=train_full.column_names)\n",
    "    tokenized_val = val_full.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=val_full.column_names)\n",
    "    tokenized_test = test_full.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=test_full.column_names)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUTPUT_DIR, f\"final_{model_name.replace('/', '_')}\"),\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        seed=SEED,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_custom\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "    # Save final model\n",
    "    trainer.save_model(os.path.join(OUTPUT_DIR, \"best_full_model\"))\n",
    "    return trainer, eval_metrics, tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(trainer: Trainer, tokenizer, raw_test_dataset: Dataset, sample_n=10):\n",
    "    print(\"\\n--- Running inference on random samples ---\")\n",
    "    rng = random.Random(SEED)\n",
    "    indices = rng.sample(range(len(raw_test_dataset)), k=min(sample_n, len(raw_test_dataset)))\n",
    "    samples = [raw_test_dataset[i] for i in indices]\n",
    "    texts = [s[\"review\"] for s in samples]\n",
    "    enc = tokenizer(texts, truncation=True, max_length=MAX_LENGTH, padding=True, return_tensors=\"pt\")\n",
    "    # move to CPU explicitly\n",
    "    outputs = trainer.model(**{k: v for k, v in enc.items()})\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    for i, txt in enumerate(texts):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(\"Review:\", txt[:400].replace(\"\\n\", \" \").strip())\n",
    "        print(\"Predicted label:\", \"positive\" if preds[i]==1 else \"negative\")\n",
    "        print(\"Logits:\", logits[i])\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"Loading dataset (local CSV)...\")\n",
    "    full_ds = load_local_imdb_csv(\"/content/IMDB Dataset.csv\")  # raises if not present\n",
    "    print(f\"Total examples loaded: {len(full_ds)}\")\n",
    "\n",
    "    print(\"Preparing subset + test split for model selection...\")\n",
    "    datasets_sub = prepare_datasets(full_ds, subset_size=NUM_EXAMPLES_SUBSET, val_size=NUM_VAL_SUBSET)\n",
    "    print(\"Subset sizes:\", {k: len(datasets_sub[k]) for k in datasets_sub})\n",
    "\n",
    "    # Iterate models (quick)\n",
    "    model_scores = {}\n",
    "    model_details = {}\n",
    "    for model_name in MODEL_NAMES:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenizer load failed for {model_name}: {e}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            f1, metrics = train_and_eval_model(model_name, datasets_sub, tokenizer=tokenizer, epochs=NUM_EPOCHS_SUBSET, run_name=f\"subset_{model_name.replace('/', '_')}\")\n",
    "            model_scores[model_name] = f1\n",
    "            model_details[model_name] = metrics\n",
    "            print(f\"Model {model_name} subset F1 = {f1:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed for {model_name}: {e}. Skipping.\")\n",
    "\n",
    "    if not model_scores:\n",
    "        print(\"No models finished training successfully. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Choose best model by F1\n",
    "    best_model = max(model_scores.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"\\nBest model on subset: {best_model} with F1={model_scores[best_model]:.4f}\")\n",
    "\n",
    "    # Final training on full dataset with best model\n",
    "    tokenizer_best = AutoTokenizer.from_pretrained(best_model, use_fast=True)\n",
    "    trainer, eval_metrics, tokenized_test = train_best_on_full(best_model, full_ds, tokenizer=tokenizer_best, epochs=NUM_EPOCHS_FULL)\n",
    "    print(f\"\\nFinal test metrics (best model on full data): {eval_metrics}\")\n",
    "\n",
    "    # Run inference on 10 random samples from test set\n",
    "    # Need raw test dataset (un-tokenized). In train_best_on_full we created tokenized_test; get the raw test split:\n",
    "    # We can reconstruct raw test by splitting full_ds again deterministically:\n",
    "    ds_shuf = full_ds.shuffle(seed=SEED)\n",
    "    n = len(ds_shuf)\n",
    "    n_train = int(n * 0.8)\n",
    "    n_val = int(n * 0.1)\n",
    "    test_raw = ds_shuf.select(range(n_train + n_val, n))\n",
    "    run_inference(trainer, tokenizer_best, test_raw, sample_n=10)\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_inference_visual(trainer: Trainer, tokenizer, raw_test_dataset: Dataset, sample_n=10):\n",
    "    print(\"\\n--- Running visual inference on random samples ---\")\n",
    "    rng = random.Random(SEED)\n",
    "    indices = rng.sample(range(len(raw_test_dataset)), k=min(sample_n, len(raw_test_dataset)))\n",
    "    samples = [raw_test_dataset[i] for i in indices]\n",
    "    texts = [s[\"review\"] for s in samples]\n",
    "    true_labels = [s[\"label\"] for s in samples]\n",
    "\n",
    "    enc = tokenizer(texts, truncation=True, max_length=MAX_LENGTH, padding=True, return_tensors=\"pt\")\n",
    "    outputs = trainer.model(**{k: v for k, v in enc.items()})\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Review (truncated)\": [t[:120].replace(\"\\n\",\" \") + (\"...\" if len(t) > 120 else \"\") for t in texts],\n",
    "        \"True Label\": [\"positive\" if l==1 else \"negative\" for l in true_labels],\n",
    "        \"Predicted\": [\"positive\" if p==1 else \"negative\" for p in preds]\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== Predictions Table ===\\n\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # Force plot to render\n",
    "    label_counts = df[\"Predicted\"].value_counts()\n",
    "    plt.figure(figsize=(5,4))\n",
    "    label_counts.plot(kind=\"bar\", color=[\"#4CAF50\", \"#F44336\"])\n",
    "    plt.title(\"Predicted Sentiment Counts (Sample of 10)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=True)   # <--- ensures the plot window actually shows up\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
