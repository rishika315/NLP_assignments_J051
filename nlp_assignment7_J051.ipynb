{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOEI61vL6un9"
   },
   "source": [
    "##**Question Answering & Named Entity Recognition with Transformer Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwXxR7_y62rS"
   },
   "source": [
    "\n",
    "### **1️⃣ Question Answering (QA) Task**\n",
    "\n",
    "* **Dataset:** Stanford Question Answering Dataset (SQuAD).\n",
    "* **Objective:** Train a transformer-based QA model to predict answer spans given a context and a question.\n",
    "* **Custom Metric:** Token-level **Intersection over Union (IoU)** to evaluate how well predicted answer spans overlap with ground truth.\n",
    "* **Pipeline:** Build an end-to-end inference pipeline for the model to handle new questions efficiently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- INSTALL DEPENDENCIES ---------\n",
    "!pip install -q transformers datasets evaluate matplotlib\n",
    "\n",
    "# --------- IMPORTS ---------\n",
    "import json, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator, pipeline\n",
    ")\n",
    "\n",
    "# --------- TOKEN-LEVEL IOU METRIC ---------\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = t.lower().strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = t.strip(\" '\\\".,;:-()[]{}\")\n",
    "    return t\n",
    "\n",
    "def token_iou(pred_text: str, gold_texts) -> float:\n",
    "    pred = normalize_text(pred_text)\n",
    "    if not pred:\n",
    "        return 0.0\n",
    "    pred_tokens = pred.split()\n",
    "    best = 0.0\n",
    "    for g in gold_texts:\n",
    "        gnorm = normalize_text(g)\n",
    "        gold_tokens = gnorm.split()\n",
    "        inter = len(set(pred_tokens) & set(gold_tokens))\n",
    "        union = len(set(pred_tokens) | set(gold_tokens))\n",
    "        if union > 0:\n",
    "            best = max(best, inter / union)\n",
    "    return best\n",
    "\n",
    "# --------- LOAD SQUAD JSON FILES ---------\n",
    "def load_squad_local(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        squad = json.load(f)\n",
    "    flat_data = []\n",
    "    for article in squad[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                flat_data.append({\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answers\": qa[\"answers\"]\n",
    "                })\n",
    "    return Dataset.from_list(flat_data)\n",
    "\n",
    "train_dataset = load_squad_local(\"train-v1.1.json\")\n",
    "val_dataset   = load_squad_local(\"dev-v1.1.json\")\n",
    "\n",
    "# --------- SUBSAMPLE FOR FAST CPU ---------\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "val_dataset   = val_dataset.select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- TOKENIZER & PREPROCESSING ---------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "MAX_LENGTH = 384\n",
    "DOC_STRIDE = 128\n",
    "\n",
    "def preprocess(examples):\n",
    "    # Tokenize the question and context\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Get mappings to original samples and character offsets\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    # Pop offset_mapping once at the beginning\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    # Process each generated feature\n",
    "    for i in range(len(tokenized[\"input_ids\"])):\n",
    "        # Get offset mapping for the current feature\n",
    "        offsets = offset_mapping[i]\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id) # Index of the CLS token\n",
    "        sample_index = sample_mapping[i] # Index of the original example this feature came from\n",
    "\n",
    "        # Get the answers for the original example\n",
    "        # examples[\"answers\"] is a list where each element corresponds to an original example\n",
    "        # examples[\"answers\"][sample_index] is the list of answer dictionaries for that specific example\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        # Default start and end positions to CLS token index (for unanswerable or features without answer)\n",
    "        current_start_position = cls_index\n",
    "        current_end_position = cls_index\n",
    "\n",
    "        # If there are answers for this example\n",
    "        if answers:\n",
    "            # For SQuAD v1.1, we only consider the first answer for training\n",
    "            first_answer = answers[0]\n",
    "            start_char = first_answer[\"answer_start\"]\n",
    "            end_char = start_char + len(first_answer[\"text\"])\n",
    "\n",
    "            # Find the start and end token indices of the answer in the context part of the feature\n",
    "            sequence_ids = tokenized.sequence_ids(i)\n",
    "            # Find the start and end of the context in the tokenized sequence\n",
    "            context_start = sequence_ids.index(1) if 1 in sequence_ids else None\n",
    "            context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1) if 1 in sequence_ids[::-1] else None\n",
    "\n",
    "            if context_start is not None and context_end is not None:\n",
    "                 # Adjust context_end to be inclusive of the last token\n",
    "                 context_end += 1\n",
    "\n",
    "                 # Find the tokenized start index of the answer\n",
    "                 token_start_index = context_start\n",
    "                 while token_start_index < context_end and offsets[token_start_index][0] <= start_char:\n",
    "                     token_start_index += 1\n",
    "                 # If the answer start is within the span of the token we stopped at (token_start_index - 1)\n",
    "                 if token_start_index < context_end and offsets[token_start_index - 1][0] <= start_char and offsets[token_start_index - 1][1] >= start_char:\n",
    "                      current_start_position = token_start_index - 1\n",
    "                 else:\n",
    "                      current_start_position = cls_index # Mark as unanswerable in this feature\n",
    "\n",
    "\n",
    "                 # Find the tokenized end index of the answer\n",
    "                 token_end_index = context_end - 1\n",
    "                 while token_end_index >= context_start and offsets[token_end_index][1] >= end_char:\n",
    "                     token_end_index -= 1\n",
    "                 # If the answer end is within the span of the token we stopped at (token_end_index + 1)\n",
    "                 if token_end_index >= context_start and offsets[token_end_index + 1][1] >= end_char and offsets[token_end_index + 1][0] <= end_char:\n",
    "                      current_end_position = token_end_index + 1\n",
    "                 else:\n",
    "                      current_end_position = cls_index # Mark as unanswerable in this feature\n",
    "\n",
    "                 # If the tokenized answer span is not fully contained within the context part of the feature\n",
    "                 # This check also implicitly handles cases where start_token > end_token\n",
    "                 if not (offsets[current_start_position][0] >= start_char and offsets[current_end_position][1] <= end_char):\n",
    "                     current_start_position = cls_index\n",
    "                     current_end_position = cls_index\n",
    "\n",
    "\n",
    "        start_positions.append(current_start_position)\n",
    "        end_positions.append(current_end_position)\n",
    "\n",
    "\n",
    "    # Add the calculated start and end positions to the tokenized inputs\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Keep overflow_to_sample_mapping if needed for post-processing during evaluation,\n",
    "    # but remove it for this simplified training setup if not used.\n",
    "    # The pop is already done at the beginning, so no need to pop again here.\n",
    "    # tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the datasets\n",
    "# Use batched=True because the preprocess function is designed to handle batches\n",
    "# remove_columns removes the original columns that are no longer needed after tokenization\n",
    "train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset   = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "# --------- KEEP ONLY REQUIRED COLUMNS ---------\n",
    "# Ensure only the columns needed for training are kept.\n",
    "# These are typically input_ids, attention_mask, start_positions, and end_positions.\n",
    "required_columns = [\"input_ids\",\"attention_mask\",\"start_positions\",\"end_positions\"]\n",
    "train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in required_columns])\n",
    "val_dataset   = val_dataset.remove_columns([c for c in val_dataset.column_names if c not in required_columns])\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- TRAINING SETUP ---------\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa-cpu-distilbert\",\n",
    "    eval_strategy=\"epoch\",  # Corrected argument name\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=None,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- TRAIN & SAVE ---------\n",
    "# Assuming trainer, train_dataset, and val_dataset are defined and configured in previous cells\n",
    "# trainer.train() # Assuming training is done in a previous cell\n",
    "# trainer.save_model(\"./qa-cpu-distilbert\") # Assuming saving is done in a previous cell\n",
    "\n",
    "# --------- INFERENCE PIPELINE ---------\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Ensure tokenizer is loaded and available\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Ensure the model is loaded for the pipeline\n",
    "# If training was done in a previous cell, the model should be saved at \"./qa-cpu-distilbert\"\n",
    "# If not, you might need to load a pre-trained model here for inference demo purposes.\n",
    "try:\n",
    "    qa_pipe = pipeline(\"question-answering\", model=\"./qa-cpu-distilbert\", tokenizer=tokenizer, device=-1)\n",
    "    print(\"QA pipeline loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading QA pipeline: {e}\")\n",
    "    print(\"Please ensure the trained model is saved at './qa-cpu-distilbert'.\")\n",
    "    # Exit or handle the error appropriately if the pipeline cannot be loaded\n",
    "    # exit()\n",
    "\n",
    "\n",
    "def infer_and_score(context, question, gold_answers=None):\n",
    "    \"\"\"\n",
    "    Run QA and compute token-IoU if gold_answers provided\n",
    "    \"\"\"\n",
    "    # Check if qa_pipe is loaded before using it\n",
    "    if 'qa_pipe' not in globals():\n",
    "        print(\"Error: QA pipeline not loaded.\")\n",
    "        return {\"answer\": \"\", \"score\": 0.0, \"token_iou\": None}\n",
    "\n",
    "    try:\n",
    "        out = qa_pipe({\"context\": context, \"question\": question})\n",
    "        pred = out[\"answer\"].strip()\n",
    "        score = out[\"score\"]\n",
    "        # Assume token_iou function is defined in a previous cell and available\n",
    "        if 'token_iou' not in globals():\n",
    "            print(\"Warning: token_iou function not found. Skipping IoU calculation.\")\n",
    "            iou = None\n",
    "        else:\n",
    "            # Pass gold_answers as a list to token_iou\n",
    "            iou = token_iou(pred, gold_answers) if gold_answers is not None else None\n",
    "        return {\"answer\": pred, \"score\": score, \"token_iou\": iou}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        return {\"answer\": \"\", \"score\": 0.0, \"token_iou\": None}\n",
    "\n",
    "\n",
    "# --------- DEMO ---------\n",
    "# Load original validation data for demo purposes\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_squad_local(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        squad = json.load(f)\n",
    "    flat_data = []\n",
    "    for article in squad[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                flat_data.append({\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answers\": qa[\"answers\"]\n",
    "                })\n",
    "    return Dataset.from_list(flat_data)\n",
    "\n",
    "# Load the original validation dataset\n",
    "original_val_dataset = load_squad_local(\"dev-v1.1.json\")\n",
    "\n",
    "# Subsample the original validation dataset for a quick demo if needed\n",
    "if len(original_val_dataset) > 20:\n",
    "     original_val_dataset = original_val_dataset.select(range(20))\n",
    "\n",
    "\n",
    "# Select an example from the original validation dataset for the demo\n",
    "if len(original_val_dataset) > 0:\n",
    "    example = original_val_dataset[0]\n",
    "    example_ctx = example[\"context\"]\n",
    "    example_q = example[\"question\"]\n",
    "    # Get the text of the first gold answer, if available\n",
    "    example_gold = example[\"answers\"][0][\"text\"] if example[\"answers\"] else \"\"\n",
    "\n",
    "    # Perform inference and score\n",
    "    res = infer_and_score(example_ctx, example_q, [example_gold]) # Pass gold_answers as a list\n",
    "\n",
    "    print(\"Question:\", example_q)\n",
    "    print(\"Gold:\", example_gold)\n",
    "    print(\"Prediction:\", res[\"answer\"])\n",
    "    print(\"Score:\", round(res[\"score\"], 4))\n",
    "    if res[\"token_iou\"] is not None:\n",
    "        print(\"Token-IoU:\", round(res[\"token_iou\"], 4))\n",
    "else:\n",
    "    print(\"Original validation dataset is empty or could not be loaded for the demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- VISUALIZATION ---------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "predictions, golds, ious = [], [], []\n",
    "\n",
    "for i in range(len(original_val_dataset)):\n",
    "    ex = original_val_dataset[i]\n",
    "    ctx, q, gold = ex[\"context\"], ex[\"question\"], ex[\"answers\"][0][\"text\"] if ex[\"answers\"] else \"\"\n",
    "    res = infer_and_score(ctx, q, [gold])\n",
    "    predictions.append(res[\"answer\"])\n",
    "    golds.append(gold)\n",
    "    ious.append(res[\"token_iou\"] if res[\"token_iou\"] is not None else 0.0)\n",
    "\n",
    "# --- Plot Token-IoU distribution ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(ious, bins=10, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Token-IoU on Validation Subset\")\n",
    "plt.xlabel(\"Token-IoU\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot IoU per example ---\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(len(ious)), ious, marker=\"o\", linestyle=\"--\", color=\"green\")\n",
    "plt.title(\"Token-IoU per Example\")\n",
    "plt.xlabel(\"Example Index\")\n",
    "plt.ylabel(\"Token-IoU\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# --- Show sample table ---\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"Question\": [original_val_dataset[i][\"question\"] for i in range(len(original_val_dataset))],\n",
    "    \"Gold Answer\": golds,\n",
    "    \"Prediction\": predictions,\n",
    "    \"Token-IoU\": np.round(ious, 3)\n",
    "})\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5ZBTT8w65GF"
   },
   "source": [
    "\n",
    "### **2️⃣ Named Entity Recognition (NER) Task**\n",
    "\n",
    "* **Dataset:** PII detection and removal dataset from educational data.\n",
    "* **Objective:** Train a DeBERTa-based NER model to detect sensitive entities.\n",
    "* **Label Processing:** Use **BIO tagging scheme**, assign `-100` to subword tokens to avoid penalizing the loss.\n",
    "* **Custom Metric:** Use **`seqeval`** to compute entity-level F1-score, precision, and recall.\n",
    "* **Inference:** Explore Hugging Face `pipeline` for NER with **aggregation strategy** to combine token-level predictions into full entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# ---------------- Load dataset ----------------\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"train.json\",\n",
    "    \"test\": \"test.json\"\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Load dataset ----------------\n",
    "# Assuming dataset loading from local json files is done in a previous cell\n",
    "# dataset = load_dataset(\"json\", data_files={\n",
    "#     \"train\": \"train.json\",\n",
    "#     \"test\": \"test.json\"\n",
    "# })\n",
    "\n",
    "# If the dataset variable is not defined from a previous cell,\n",
    "# uncomment the above lines or ensure the dataset is loaded elsewhere.\n",
    "# For this fix, we assume the dataset variable is already defined.\n",
    "\n",
    "# ---------------- Preprocess: tokenize text into tokens ----------------\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\") # Download the punkt_tab resource\n",
    "\n",
    "# Here we create dummy ner_tags = \"O\" for all tokens\n",
    "# Replace this with your real BIO-tagged labels if available.\n",
    "def create_tokens_and_labels(example):\n",
    "    # Ensure 'full_text' is in the example\n",
    "    if \"full_text\" not in example:\n",
    "        # Handle cases where 'full_text' is missing or is not a string\n",
    "        print(f\"Warning: 'full_text' not found in example or is not a string. Skipping example.\")\n",
    "        return {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    # Ensure 'full_text' is a string before tokenizing\n",
    "    if not isinstance(example[\"full_text\"], str):\n",
    "         print(f\"Warning: 'full_text' is not a string in example. Skipping example.\")\n",
    "         return {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(example[\"full_text\"])\n",
    "    except LookupError as e:\n",
    "        print(f\"Error during tokenization: {e}. Please ensure NLTK data is downloaded.\")\n",
    "        return {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    # Assuming 'labels' are provided in the original dataset\n",
    "    # If labels are available, align them with tokens here.\n",
    "    # If only 'full_text' and 'tokens' are available, generate dummy 'O' tags.\n",
    "    # Add check for example[\"labels\"] is not None\n",
    "    if \"labels\" in example and example[\"labels\"] is not None and len(example[\"labels\"]) == len(tokens):\n",
    "         # Assuming labels are already in the desired format (e.g., list of tags)\n",
    "         labels = example[\"labels\"]\n",
    "    else:\n",
    "        # Generate dummy tags if real labels are not available, are None, or don't match token count\n",
    "        labels = [\"O\"] * len(tokens)  # dummy tags\n",
    "\n",
    "\n",
    "    return {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "\n",
    "# Check if dataset is loaded before mapping\n",
    "if 'dataset' not in globals():\n",
    "    print(\"Error: 'dataset' variable not found. Please ensure the dataset is loaded in a previous cell.\")\n",
    "else:\n",
    "    # Apply the mapping function\n",
    "    dataset = dataset.map(create_tokens_and_labels, load_from_cache_file=False) # Disable cache to re-run\n",
    "\n",
    "\n",
    "# ---------------- Labels ----------------\n",
    "# Define label list based on the dataset or task\n",
    "# Adjust if your dataset has different entity tags\n",
    "# Assuming \"O\", \"B-PII\", \"I-PII\" based on the markdown cell description\n",
    "label_list = [\"O\", \"B-PII\", \"I-PII\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "# Map string tags -> ids\n",
    "def encode_labels(example):\n",
    "    # Ensure 'ner_tags' is in the example and is a list\n",
    "    if \"ner_tags\" not in example or not isinstance(example[\"ner_tags\"], list):\n",
    "         print(f\"Warning: 'ner_tags' not found or is not a list in example. Skipping label encoding.\")\n",
    "         return {\"ner_tags\": []} # Return empty list to avoid errors\n",
    "\n",
    "    # Encode labels using label_to_id, handling potential missing tags if necessary\n",
    "    encoded_tags = []\n",
    "    for tag in example[\"ner_tags\"]:\n",
    "        if tag in label_to_id:\n",
    "             encoded_tags.append(label_to_id[tag])\n",
    "        else:\n",
    "             # Handle unexpected tags, e.g., map to \"O\" or skip\n",
    "             print(f\"Warning: Unknown NER tag '{tag}' found. Mapping to 'O'.\")\n",
    "             encoded_tags.append(label_to_id[\"O\"]) # Map unknown tags to \"O\"\n",
    "\n",
    "    return {\"ner_tags\": encoded_tags}\n",
    "\n",
    "# Check if dataset is loaded before mapping\n",
    "if 'dataset' not in globals():\n",
    "    print(\"Error: 'dataset' variable not found. Please ensure the dataset is loaded in a previous cell.\")\n",
    "else:\n",
    "     # Apply the encoding function\n",
    "     dataset = dataset.map(encode_labels, load_from_cache_file=False) # Disable cache to re-run\n",
    "\n",
    "\n",
    "# ---------------- Tokenizer ----------------\n",
    "model_name = \"microsoft/deberta-v3-small\"  # long context support\n",
    "# Ensure tokenizer is loaded and available\n",
    "if 'tokenizer' not in globals() or tokenizer.name_or_path != model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Process batches of examples\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True, # Crucial for aligning with word-level labels\n",
    "        padding=\"max_length\",\n",
    "        max_length=512 # Align with model's max input length\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    # Iterate over examples in the batch\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Map tokens to original words\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Iterate over tokens in the tokenized input\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens (like [CLS], [SEP], [PAD]) get a label of -100\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # We are at the start of a new word or a word that is split into multiple tokens.\n",
    "                # The label for the first token of a word is the label of the word.\n",
    "                # Ensure word_idx is within the bounds of the original word labels\n",
    "                if word_idx < len(label):\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    # Handle cases where word_idx is out of bounds for the original labels (should not happen with is_split_into_words=True and correct data)\n",
    "                    label_ids.append(-100) # Treat as special token or unlabelled\n",
    "            else:\n",
    "                # For subsequent tokens of a word, we assign -100 to ignore them in the loss calculation\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Check if dataset is loaded and has required splits before mapping\n",
    "if 'dataset' not in globals():\n",
    "     print(\"Error: 'dataset' variable not found. Skipping tokenization and alignment.\")\n",
    "else:\n",
    "    if \"train\" in dataset:\n",
    "         tokenized_train = dataset[\"train\"].map(tokenize_and_align_labels, batched=True, load_from_cache_file=False) # Disable cache to re-run\n",
    "         print(\"Tokenized Training Dataset:\", tokenized_train)\n",
    "    else:\n",
    "         print(\"Error: 'train' split not found in dataset.\")\n",
    "\n",
    "    if \"test\" in dataset:\n",
    "         tokenized_test = dataset[\"test\"].map(tokenize_and_align_labels, batched=True, load_from_cache_file=False) # Disable cache to re-run\n",
    "         print(\"Tokenized Test Dataset:\", tokenized_test)\n",
    "    else:\n",
    "         print(\"Error: 'test' split not found in dataset.\")\n",
    "\n",
    "\n",
    "# --------- KEEP ONLY REQUIRED COLUMNS ---------\n",
    "# Ensure only the columns needed for training are kept.\n",
    "# These are typically input_ids, attention_mask, and labels.\n",
    "required_columns = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "\n",
    "if 'tokenized_train' in globals():\n",
    "    tokenized_train = tokenized_train.remove_columns([c for c in tokenized_train.column_names if c not in required_columns])\n",
    "    print(\"Final Training Dataset:\", tokenized_train)\n",
    "else:\n",
    "    print(\"Tokenized training dataset not available to remove columns.\")\n",
    "\n",
    "if 'tokenized_test' in globals():\n",
    "    tokenized_test = tokenized_test.remove_columns([c for c in tokenized_test.column_names if c not in required_columns])\n",
    "    print(\"Final Test Dataset:\", tokenized_test)\n",
    "else:\n",
    "     print(\"Tokenized test dataset not available to remove columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assume label_list is defined elsewhere, e.g., in the preprocessing cell\n",
    "# label_list = [\"O\", \"B-PII\", \"I-PII\"]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100) for both preds and labels\n",
    "    # Also, ensure predicted indices are within the valid range of label_list\n",
    "    true_preds = [\n",
    "        [label_list[p] for (p, l) in zip(pred_row, label_row) if l != -100 and p < len(label_list)] # Add check for valid index\n",
    "        for pred_row, label_row in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(pred_row, label_row) if l != -100]\n",
    "        for pred_row, label_row in zip(preds, labels)\n",
    "    ]\n",
    "\n",
    "    # Handle cases where true_labels might be empty after filtering\n",
    "    if not true_labels or all(not sublist for sublist in true_labels):\n",
    "        # If there are no true labels to evaluate against, return 0 for metrics\n",
    "        return {\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1\": 0.0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "\n",
    "# -------- Disable WandB + Warnings --------\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -------- Model --------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_list)\n",
    ")\n",
    "\n",
    "# -------- Training Args (FAST) --------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_deberta\",\n",
    "    eval_strategy=\"epoch\",    # correct arg\n",
    "    save_strategy=\"no\",             # don't waste time saving checkpoints each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,  # lighter for CPU\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,             # 1 epoch only (fastest demo)\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"no\",          # silence logging\n",
    "    report_to=\"none\"                # no wandb / tensorboard\n",
    ")\n",
    "\n",
    "# -------- Trainer --------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------- Train + Save --------\n",
    "trainer.train()\n",
    "trainer.save_model(\"./ner_deberta\")\n",
    "\n",
    "# -------- Inference Pipeline --------\n",
    "ner_pipe = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"./ner_deberta\",\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",  # merges B/I into full entity\n",
    "    device=-1                       # force CPU\n",
    ")\n",
    "\n",
    "# -------- Demo --------\n",
    "text = \"My name is John Doe and my phone number is 123-456-7890.\"\n",
    "results = ner_pipe(text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_path = \"/content/nlp_assignment7_J051.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Remove the 'widgets' metadata if present\n",
    "if \"widgets\" in nb.get(\"metadata\", {}):\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    json.dump(nb, f, indent=1)\n",
    "\n",
    "print(\"Widget metadata removed. Safe to upload to GitHub.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
